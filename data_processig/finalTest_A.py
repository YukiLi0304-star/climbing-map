import requests
import json
import re
import time
from bs4 import BeautifulSoup
import urllib.parse

MANUAL_COORDS = {
    "Dalkey Quarry": {"latitude": 53.272, "longitude": -6.108},
    "Fair Head": {"latitude": 55.216, "longitude": -6.136},
    "Glendalough": {"latitude": 53.012, "longitude": -6.355},
    "Burren": {"latitude": 53.056, "longitude": -9.167},
}

class IrishClimbingRobust:
    def __init__(self):
        self.base_url = "http://wiki.climbing.ie"
        self.api_url = "http://wiki.climbing.ie/api.php"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

    def get_all_counties_and_sites_via_scraping(self):
        print("é€šè¿‡ç½‘é¡µçˆ¬å–è·å–éƒ¡å’Œæ”€å²©ç‚¹åˆ—è¡¨...")
        try:
            response = self.session.get(f"{self.base_url}/index.php?title=Irish_Climbing_Wiki")
            soup = BeautifulSoup(response.content, 'html.parser')
            all_data = {}
            current_county = None

            for element in soup.find_all(['h1', 'h2', 'h3', 'ul', 'p']):
                if element.name in ['h1', 'h2', 'h3']:
                    text = element.get_text().strip()
                    text = re.sub(r'\[edit\]', '', text).strip()
                    if text.startswith('Co. ') and len(text) > 5:
                        current_county = text
                        all_data[current_county] = {'county_info': {'name': current_county}, 'climbing_sites': []}
                        print(f"\næ‰¾åˆ°éƒ¡: {current_county}")

                elif element.name in ['ul', 'p'] and current_county:
                    links = element.find_all('a', href=True)
                    for link in links:
                        text = link.get_text().strip()
                        href = link['href']
                        if self._is_valid_climbing_site(text, href):
                            page_title = self._extract_page_title(href)
                            site_data = {
                                'name': text,
                                'page_title': page_title,
                                'url': f"{self.base_url}{href}" if href.startswith('/') else href
                            }
                            all_data[current_county]['climbing_sites'].append(site_data)
                            print(f"  âœ“ {text}")
            return all_data
        except Exception as e:
            print(f"ç½‘é¡µçˆ¬å–å¤±è´¥: {e}")
            return {}

    def _extract_routes_section(self, full_text: str) -> str:
        if not full_text: return ""
        pattern = r'==+\s*(Routes?|Climbs?)\s*==+'
        m = re.search(pattern, full_text, flags=re.IGNORECASE)
        if not m: return full_text
        start = m.end()
        m2 = re.search(r'\n==[^=].*?==', full_text[start:], flags=re.IGNORECASE)
        if m2: return full_text[start:start + m2.start()]
        return full_text[start:]

    def _wikitext_to_plain(self, text: str) -> str:
        if not text:
            return ""

        text = re.sub(r'', '', text, flags=re.DOTALL)
        text = re.sub(r'\{\{.*?\}\}', '', text, flags=re.DOTALL)

        text = re.sub(r'<(?:br|p|div)\s*/?>', '\n', text, flags=re.IGNORECASE)
        text = re.sub(r'</(?:p|div)>', '\n', text, flags=re.IGNORECASE)
        
        text = re.sub(r'</?(?:b|strong|i|em|span)[^>]*>', ' ', text, flags=re.IGNORECASE)

        text = re.sub(r'\[\[(?:[^|\]]*\|)?([^\]]+)\]\]', r'\1', text)
        
        text = re.sub(r'\[(?:https?://[^\s]+)\s+([^\]]+)\]', r'\1', text)

        text = re.sub(r"''+", '', text)

        text = re.sub(r'[ \t]+', ' ', text)
        text = re.sub(r'\n\s*\n+', '\n', text)

        return text.strip()


    def clean_route_name(self, name: str) -> str:
        if not name: return ""
        name = re.sub(r'^\s*\d+[a-zA-Z]?\.?\s*', '', name)
        cleaned = re.sub(r'[\*\#\|\-]+', ' ', name)
        cleaned = re.sub(r'\s+', ' ', cleaned)
        cleaned = cleaned.strip(' \t\n\r\"\'')
        return cleaned

    def _looks_like_grade(self, difficulty: str) -> bool:
        if not difficulty: return False
        d = difficulty.strip()
        uk_trad_grades = {"M", "MS","D", "VD", "Diff", "VDiff", "HVD", "S", "HS", "VS", "HVS", "E1", "E2", "E3", "E4", "E5", "E6", "E7", "E8", "E9"}
        if d in uk_trad_grades: return True
        if re.fullmatch(r"E[1-9](/[1-9])?", d): return True
        if re.fullmatch(r"[3-9][abc][+-]?", d.lower()): return True
        if re.fullmatch(r"[3-9][ABCD][+-]?", d): return True
        return False

    #def _parse_route_line(self, line: str):
        if not line: return None
        line = line.strip()
        
        if re.search(r'(19|20)\d{2}', line) or re.search(r'\d{1,2}/\d{1,2}', line):
            return None

        if re.search(r'[A-Z]\s+[A-Z][a-z]+', line) and "," in line:
            return None

        if len(line) > 80: return None
        ignore_starts = ["A small", "The", "This", "Located", "Situated", "Access", "Descent", "Approach", "Takes"]
        for start_word in ignore_starts:
            if line.startswith(start_word):
                return None

        line = re.sub(r'^\s*\d+[a-zA-Z]?\.?\s*', '', line)
        line = line.strip()
        if not line: return None
        tokens = line.split()
        if len(tokens) < 2: return None
        
        height = None
        height_idx = None
        for i, tok in enumerate(tokens):
            m = re.fullmatch(r'(\d{1,3})m', tok)
            if m:
                height = int(m.group(1))
                height_idx = i
                break
        
        grade_indices = []
        start_search = height_idx + 1 if height_idx is not None else 1
        for i in range(start_search, len(tokens)):
            if self._looks_like_grade(tokens[i].strip("()")):
                grade_indices.append(i)
        if not grade_indices:
            for i in range(1, len(tokens)):
                if self._looks_like_grade(tokens[i].strip("()")):
                    grade_indices.append(i)
                    break
        if not grade_indices: return None

        overall_grade = tokens[grade_indices[0]].strip("()")
        technical_grade = None
        if len(grade_indices) > 1:
            technical_grade = tokens[grade_indices[1]].strip("()")
        else:
            for i in range(grade_indices[0] + 1, len(tokens)):
                cand = tokens[i].strip("()")
                if self._looks_like_grade(cand):
                    technical_grade = cand
                    break

        cut_positions = []
        if height_idx is not None: cut_positions.append(height_idx)
        if grade_indices: cut_positions.append(grade_indices[0])
        if not cut_positions: return None

        name_end = min(cut_positions)
        name_tokens = [t for t in tokens[:name_end] if t not in ['*', '**']]
        raw_name = " ".join(name_tokens)
        route_name = self.clean_route_name(raw_name)

        if len(route_name) > 40: return None
        if len(route_name) < 2: return None
        if (not route_name or route_name.lower() in ['the', 'and', 'or', 'if', 'start']): return None

        difficulty_str = overall_grade
        if technical_grade and technical_grade != overall_grade:
            difficulty_str = f"{overall_grade} {technical_grade}"

        return {
            'name': route_name,
            'height': height,
            'overall_grade': overall_grade,
            'technical_grade': technical_grade,
            'difficulty': difficulty_str,
        }
    
    def _parse_route_line(self, line: str):
        """é‡å†™ - é€‚é…wikitextæ ¼å¼"""
        if not line:
            return None
        
        line = line.strip()
        
        # ç§»é™¤wikitextæ ‡è®°
        line = re.sub(r"''+", '', line)  # ç§»é™¤ç²—ä½“/æ–œä½“æ ‡è®°
        line = re.sub(r'\[\[[^|\]]*\|([^\]]+)\]\]', r'\1', line)  # è½¬æ¢[[é“¾æ¥|æ˜¾ç¤ºæ–‡æœ¬]]ä¸ºæ˜¾ç¤ºæ–‡æœ¬
        line = re.sub(r'\[\[([^\]]+)\]\]', r'\1', line)  # è½¬æ¢[[é¡µé¢å]]ä¸ºé¡µé¢å
        
        # è·³è¿‡å¤ªé•¿çš„è¡Œï¼ˆé€šå¸¸æ˜¯æ®µè½ï¼‰
        if len(line) > 100:
            return None
        
        # è·³è¿‡æ˜æ˜¾çš„éè·¯çº¿è¡Œ
        if line.startswith(('The ', 'This ', 'Located ', 'Situated ', 'Access ', 
                            'Descent ', 'Approach ', 'Takes ', 'From ', 'To ',
                            'First ', 'FA:', 'Ascent')):
            return None
        
        # ç§»é™¤æ•°å­—ç¼–å·ï¼ˆ1., 2), 3.ç­‰ï¼‰
        line = re.sub(r'^\s*\d+[\.\)]\s*', '', line)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«é«˜åº¦ï¼ˆå¿…é¡»ï¼‰
        height_match = re.search(r'(\d+)\s*m', line, re.IGNORECASE)
        if not height_match:
            return None
        
        height = int(height_match.group(1))
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«éš¾åº¦ï¼ˆå¿…é¡»ï¼‰- çˆ±å°”å…°æ”€å²©å¸¸ç”¨éš¾åº¦ç­‰çº§
        grade_patterns = [
            r'\b(E[1-9][0-9]?)\b',  # E1, E2, E3...
            r'\b(HVS|VS|HS|S|VD|HVD|D|MS|M)\b',  # ä¼ ç»Ÿéš¾åº¦
            r'\b([1-9][abc])\b',  # æŠ€æœ¯éš¾åº¦ 5a, 6bç­‰
            r'\b(F[0-9][abc])\b',  # æ³•å›½éš¾åº¦
            r'\b([A-Z][0-9])\b',  # A1, A2ç­‰
        ]
        
        grade = None
        for pattern in grade_patterns:
            grade_match = re.search(pattern, line, re.IGNORECASE)
            if grade_match:
                grade = grade_match.group(1).upper()
                break
        
        if not grade:
            return None
        
        # æå–è·¯çº¿åï¼ˆé«˜åº¦ä¹‹å‰çš„éƒ¨åˆ†ï¼‰
        height_pos = height_match.start()
        name_part = line[:height_pos].strip()
        
        # æ¸…ç†è·¯çº¿å
        name_part = re.sub(r'^[\*\-\:\s]+', '', name_part)  # ç§»é™¤å¼€å¤´çš„ç‰¹æ®Šå­—ç¬¦
        name_part = re.sub(r'[\*\-\:\s]+$', '', name_part)  # ç§»é™¤ç»“å°¾çš„ç‰¹æ®Šå­—ç¬¦
        
        # å¦‚æœè·¯çº¿åå¤ªçŸ­æˆ–å¤ªé•¿ï¼Œè·³è¿‡
        if len(name_part) < 2 or len(name_part) > 60:
            return None
        
        # æ£€æŸ¥è·¯çº¿åæ˜¯å¦æœ‰æ•ˆ
        if re.match(r'^[\d\s]+$', name_part):  # çº¯æ•°å­—
            return None
        
        return {
            'name': name_part,
            'height': height,
            'difficulty': grade,
            'overall_grade': grade,
            'technical_grade': None,
        }

    def _determine_crag_type(self, wikitext):
        """å…³é”®è¯æƒé‡æ‰“åˆ†æ³•åˆ¤æ–­ç±»å‹"""
        text_lower = wikitext.lower()
        scores = {"bouldering": 0, "sea_cliff": 0, "sport_climbing": 0, "quarry": 0}

        if "boulder" in text_lower: scores["bouldering"] += 3
        if "problem" in text_lower: scores["bouldering"] += 2
        if "sit start" in text_lower: scores["bouldering"] += 2
        if "font" in text_lower: scores["bouldering"] += 1
        
        if "sea cliff" in text_lower: scores["sea_cliff"] += 5
        if "tidal" in text_lower: scores["sea_cliff"] += 3
        if "high tide" in text_lower: scores["sea_cliff"] += 2
        
        if "sport climbing" in text_lower: scores["sport_climbing"] += 5
        if "bolted" in text_lower: scores["sport_climbing"] += 3
        if "bolts" in text_lower: scores["sport_climbing"] += 2
        
        if "quarry" in text_lower: scores["quarry"] += 5

        best_type = max(scores, key=scores.get)
        max_score = scores[best_type]

        if max_score < 2: return "Trad Climbing"
        if best_type == "bouldering": return "Bouldering"
        if best_type == "sea_cliff": return "Sea Cliff"
        if best_type == "sport_climbing": return "Sport Climbing"
        if best_type == "quarry": return "Quarry"
        return "Trad Climbing"


    #def _fetch_coords_from_osm(self, query):
        clean_query = query.replace('_', ' ').strip()
        
        search_url = "https://nominatim.openstreetmap.org/search"
        
        params = {
            'q': clean_query + ", Ireland",
            'format': 'json',
            'limit': 1
        }

        headers = {
            'User-Agent': 'CragMap_Student_App/1.0 (climbing_student_project@gmail.com)',
            'Referer': 'http://wiki.climbing.ie/'
        }
        
        try:
            print(f"      å°è¯•å» OSM æœç´¢: '{clean_query}' ...")
            
            time.sleep(1.5) 
            
            resp = requests.get(search_url, params=params, headers=headers, timeout=10)

            if resp.status_code != 200:
                print(f"     OSM æ‹’ç»è®¿é—® (ä»£ç  {resp.status_code})ã€‚å¯èƒ½æ˜¯ User-Agent é—®é¢˜ã€‚")
                return None

            data = resp.json()
            
            if data and len(data) > 0:
                lat = float(data[0]['lat'])
                lng = float(data[0]['lon'])
                print(f"     [OSMæˆåŠŸ] {clean_query} -> {lat}, {lng}")
                return {"latitude": lat, "longitude": lng}
            else:
                print(f"     OSM æ²¡æ‰¾åˆ°è¿™ä¸ªåœ°ç‚¹")
                
        except json.JSONDecodeError:
            print(f"     OSM è¿”å›æ ¼å¼é”™è¯¯ (å¯èƒ½è¢«é˜²ç«å¢™æ‹¦æˆª)")
        except Exception as e:
            print(f"      OSM æœç´¢æŠ¥é”™: {e}")
            
        return None
    
    
    
    def _guess_county_from_query(self, query):
        """ä»æŸ¥è¯¢ä¸­çŒœæµ‹éƒ¡å"""
        # å¸¸è§çš„çˆ±å°”å…°éƒ¡
        counties = ["Antrim", "Dublin", "Wicklow", "Cork", "Clare", "Galway", "Kerry", "Donegal"]
        
        query_lower = query.lower()
        
        # æ£€æŸ¥æŸ¥è¯¢ä¸­æ˜¯å¦åŒ…å«éƒ¡å
        for county in counties:
            if county.lower() in query_lower:
                return county
        
        # é»˜è®¤è¿”å›Antrimï¼ˆå› ä¸ºå¤§éƒ¨åˆ†æµ‹è¯•æ˜¯Antrimï¼‰
        return "Antrim"

    def _fetch_coords_from_osm(self, query):
        """å¼ºåˆ¶è¿”å›åæ ‡ - æ‰¾ä¸åˆ°ç²¾ç¡®çš„å°±è¿”å›çˆ±å°”å…°çš„å¤§è‡´ä½ç½®"""
        try:
            clean_query = query.replace('_', ' ').strip()
            
            # å°è¯•ç²¾ç¡®æœç´¢
            print(f"      æœç´¢: '{clean_query}'")
            
            # å…ˆå°è¯•ç›´æ¥æœç´¢
            url = "https://nominatim.openstreetmap.org/search"
            params = {
                'q': f"{clean_query}, Ireland",
                'format': 'json',
                'limit': 5,
                'addressdetails': 1
            }
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            time.sleep(1)
            response = requests.get(url, params=params, headers=headers, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                
                if data:
                    # ä¼˜å…ˆé€‰æ‹©è‡ªç„¶ç‰¹å¾æˆ–åŒ…å«æŸ¥è¯¢è¯çš„
                    for result in data:
                        display_name = str(result.get('display_name', '')).lower()
                        result_type = str(result.get('type', '')).lower()
                        
                        # æ£€æŸ¥æ˜¯å¦åœ¨çˆ±å°”å…°
                        if any(word in display_name for word in ['ireland', 'irish', 'co.']):
                            coords = {
                                "latitude": float(result['lat']),
                                "longitude": float(result['lon'])
                            }
                            print(f"      æ‰¾åˆ°ç²¾ç¡®åæ ‡: {coords}")
                            print(f"          åœ°ç‚¹: {display_name[:60]}...")
                            return coords
                    
                    # å¦‚æœæ²¡æœ‰åœ¨çˆ±å°”å…°çš„ï¼Œè‡³å°‘è¿”å›ç¬¬ä¸€ä¸ªç»“æœ
                    first_result = data[0]
                    coords = {
                        "latitude": float(first_result['lat']),
                        "longitude": float(first_result['lon'])
                    }
                    print(f"     æ‰¾åˆ°åæ ‡ï¼ˆå¯èƒ½ä¸åœ¨çˆ±å°”å…°ï¼‰: {coords}")
                    return coords
            
            # å¦‚æœç²¾ç¡®æœç´¢å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æœç´¢
            print(f"      ç²¾ç¡®æœç´¢å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨ç­–ç•¥...")
            
            # ç­–ç•¥1ï¼šæœç´¢"climbing crag"
            if 'climbing' not in clean_query.lower():
                backup_params = {'q': f"{clean_query} climbing, Ireland", 'format': 'json', 'limit': 1}
                time.sleep(1)
                response = requests.get(url, params=backup_params, headers=headers, timeout=10)
                
                if response.status_code == 200:
                    data = response.json()
                    if data:
                        coords = {
                            "latitude": float(data[0]['lat']),
                            "longitude": float(data[0]['lon'])
                        }
                        print(f"     é€šè¿‡climbingå…³é”®è¯æ‰¾åˆ°åæ ‡: {coords}")
                        return coords
            
            # ç­–ç•¥2ï¼šåªæœç´¢éƒ¡åï¼ˆä»æŸ¥è¯¢ä¸­æå–æˆ–çŒœæµ‹ï¼‰
            county = self._guess_county_from_query(query)
            if county:
                county_params = {'q': f"{county}, Ireland", 'format': 'json', 'limit': 1}
                time.sleep(1)
                response = requests.get(url, params=county_params, headers=headers, timeout=10)
                
                if response.status_code == 200:
                    data = response.json()
                    if data:
                        coords = {
                            "latitude": float(data[0]['lat']),
                            "longitude": float(data[0]['lon'])
                        }
                        print(f"    ä½¿ç”¨{county}éƒ¡ä¸­å¿ƒåæ ‡: {coords}")
                        return None
            
            # ç­–ç•¥3ï¼šè¿”å›çˆ±å°”å…°çš„åœ°ç†ä¸­å¿ƒï¼ˆä¿åº•ï¼‰
            ireland_center = {"latitude": 53.3498, "longitude": -6.2603}  # éƒ½æŸæ—
            print(f"      ğŸ´ó §ó ¢ó ©ó ¥ó ¿ ä½¿ç”¨çˆ±å°”å…°ä¸­å¿ƒåæ ‡: {ireland_center}")
            return None
            
        except Exception as e:
            print(f"     æœç´¢å‡ºé”™: {e}")
            # å³ä½¿å‡ºé”™ä¹Ÿè¿”å›çˆ±å°”å…°ä¸­å¿ƒ
            return None
        
    def extract_grid_ref_coords(self, wikitext):
        """ä»wikiæ–‡æœ¬ä¸­æå–Grid Refå¹¶è½¬æ¢ä¸ºåæ ‡"""
        print(f"      [Grid Ref] å¼€å§‹æå–...")
        
        if not wikitext:
            print(f"      [Grid Ref] wikitextä¸ºç©º")
            return None
        
        # 1. æå–Grid Refå­—ç¬¦ä¸²
        print(f"      [Grid Ref] è°ƒç”¨_extract_grid_ref...")
        grid_ref = self._extract_grid_ref(wikitext)
        
        if not grid_ref:
            print(f"      [Grid Ref] æœªæ‰¾åˆ°Grid Refå­—ç¬¦ä¸²")
            # è°ƒè¯•ï¼šæ˜¾ç¤ºwikitextçš„å‰200å­—ç¬¦ï¼Œçœ‹çœ‹é‡Œé¢æœ‰ä»€ä¹ˆ
            print(f"      [Grid Ref] wikitexté¢„è§ˆ: '{wikitext[:200]}'")
            return None
        
        print(f"      [Grid Ref] æ‰¾åˆ°Grid Ref: {grid_ref}")
        
        # 2. è½¬æ¢ä¸ºåæ ‡
        print(f"      [Grid Ref] è°ƒç”¨_convert_gridref...")
        coords = self._convert_gridref(grid_ref)
        if coords:
            print(f"      [Grid Ref] è½¬æ¢æˆåŠŸ: {coords}")
            coords["source"] = "grid_reference"
            return coords
        else:
            print(f"      [Grid Ref] è½¬æ¢å¤±è´¥")
            return None

    def _extract_grid_ref(self, text):
        """æå–Grid Refå­—ç¬¦ä¸²"""
        print(f"        [æå–Grid Ref] æ–‡æœ¬é•¿åº¦: {len(text)}")
        
        if not text:
            print(f"        [æå–Grid Ref] æ–‡æœ¬ä¸ºç©º")
            return None
        
        # å…ˆæ˜¾ç¤ºæ–‡æœ¬å¼€å¤´ï¼Œçœ‹çœ‹æœ‰ä»€ä¹ˆ
        preview = text[:200].replace('\n', ' ').replace('\r', ' ')
        print(f"        [æå–Grid Ref] æ–‡æœ¬é¢„è§ˆ: '{preview}...'")
        
        patterns = [
            r'Grid\s*Ref\.?\s*:?\s*([A-Z]{1,2}\s*\d{6})',
            r'Grid\s*Reference\.?\s*:?\s*([A-Z]{1,2}\s*\d{6})',
            r'OS\s*Grid\s*Ref\.?\s*:?\s*([A-Z]{1,2}\s*\d{6})',
            r'OS\s*Grid\s*Reference\.?\s*:?\s*([A-Z]{1,2}\s*\d{6})',
            r'([A-Z]{1,2}\s*\d{6})\s*\(OS\s*Grid\)',
            r'([A-Z]{1,2}\s*\d{3}\s*\d{3})',
            r'([A-Z]{1,2}\s*\d{5,6})',
        ]
        
        for i, pattern in enumerate(patterns):
            print(f"        [æå–Grid Ref] å°è¯•æ¨¡å¼{i+1}: {pattern[:30]}...")
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                grid_ref = match.group(1).replace(' ', '').strip()
                if len(grid_ref) >= 6:  # åŸºæœ¬éªŒè¯
                    print(f"        [æå–Grid Ref] âœ“ æ¨¡å¼{i+1}åŒ¹é…æˆåŠŸ: {grid_ref}")
                    return grid_ref.upper()
            else:
                print(f"        [æå–Grid Ref] âœ— æ¨¡å¼{i+1}æœªåŒ¹é…")
        
        print(f"        [æå–Grid Ref] âœ— æ‰€æœ‰æ¨¡å¼éƒ½æœªåŒ¹é…")
        return None

    def _convert_gridref(self, grid_ref):
        """è½¬æ¢Grid Refä¸ºç»çº¬åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        try:
            # å…ˆå°è¯•åœ¨çº¿APIè½¬æ¢
            api_coords = self._convert_gridref_via_api(grid_ref)
            if api_coords:
                return api_coords
            
            # å¦‚æœAPIå¤±è´¥ï¼Œå°è¯•æœ¬åœ°è½¬æ¢ï¼ˆé’ˆå¯¹åŒ—çˆ±å°”å…°çš„H/Iç½‘æ ¼ï¼‰
            return self._convert_gridref_locally(grid_ref)
            
        except Exception as e:
            print(f"      è½¬æ¢é”™è¯¯: {e}")
            return None

    def _convert_gridref_via_api(self, grid_ref):
        """ä½¿ç”¨åœ¨çº¿APIè½¬æ¢Grid Ref"""
        try:
            # å°è¯•gridreferencefinder.com
            url = "https://gridreferencefinder.com/gridRefAjax.php"
            params = {'gridref': grid_ref}
            headers = {'User-Agent': 'Mozilla/5.0'}
            
            print(f"      è°ƒç”¨åœ¨çº¿APIè½¬æ¢: {grid_ref}")
            response = requests.get(url, params=params, headers=headers, timeout=10)
            
            if response.status_code == 200:
                text = response.text
                
                # è§£æAPIå“åº”
                lat_match = re.search(r'Latitude[:\s]*([\d\.\-]+)', text, re.IGNORECASE)
                lon_match = re.search(r'Longitude[:\s]*([\d\.\-]+)', text, re.IGNORECASE)
                
                if lat_match and lon_match:
                    lat = float(lat_match.group(1))
                    lon = float(lon_match.group(1))
                    return {"latitude": lat, "longitude": lon}
                    
        except Exception as e:
            print(f"      APIè½¬æ¢å¤±è´¥: {e}")
        
        return None

    def _convert_gridref_locally(self, grid_ref):
        """æœ¬åœ°è½¬æ¢Grid Refï¼ˆé’ˆå¯¹åŒ—çˆ±å°”å…°ï¼‰"""
        try:
            # åªå¤„ç†åŒ—çˆ±å°”å…°çš„H/Iç½‘æ ¼
            if not grid_ref.startswith(('H', 'I')):
                print(f"      ä»…æ”¯æŒåŒ—çˆ±å°”å…°ç½‘æ ¼(H/I)")
                return None
            
            # æ¸…ç†ç½‘æ ¼å‚è€ƒ
            grid_ref = grid_ref.upper().replace(' ', '')
            
            if len(grid_ref) != 7:  # H + 6ä½æ•°å­—
                print(f"      ç½‘æ ¼æ ¼å¼é”™è¯¯: {grid_ref}")
                return None
            
            letter = grid_ref[0]
            numbers = grid_ref[1:]
            
            # Hç½‘æ ¼çš„åŸºç¡€åæ ‡ï¼ˆåŒ—çˆ±å°”å…°ï¼‰
            if letter == 'H':
                base_easting = 200000  # 500kmæ–¹æ ¼
                base_northing = 400000
            elif letter == 'I':
                base_easting = 300000
                base_northing = 400000
            else:
                return None
            
            # æå–ä¸œè·å’ŒåŒ—è·ï¼ˆ6ä½æ•°å­—ï¼šå‰3å3ï¼‰
            easting_str = numbers[:3]
            northing_str = numbers[3:]
            
            easting = base_easting + int(easting_str) * 10  # è½¬æ¢ä¸ºç±³
            northing = base_northing + int(northing_str) * 10
            
            # ç®€åŒ–çš„åæ ‡è½¬æ¢å…¬å¼ï¼ˆè¿‘ä¼¼ï¼‰
            # æ³¨æ„ï¼šè¿™æ˜¯ç®€åŒ–è½¬æ¢ï¼Œç²¾ç¡®è½¬æ¢éœ€è¦ä¸“ä¸šåº“
            lon = -6.5 + (easting - 250000) / 150000
            lat = 54.5 + (northing - 450000) / 150000
            
            # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
            lat = max(53.0, min(55.5, lat))
            lon = max(-8.0, min(-5.5, lon))
            
            return {
                "latitude": round(lat, 6),
                "longitude": round(lon, 6)
            }
            
        except Exception as e:
            print(f"      æœ¬åœ°è½¬æ¢å¤±è´¥: {e}")
            return None    

    #def _extract_coordinates_logic(self, title, wikitext):
        if title in MANUAL_COORDS:
            return MANUAL_COORDS[title]

        try:
            coord_pattern = r"\{\{[Cc]oord\|([0-9\.]+)\|([0-9\.\-]+)\}\}"
            match = re.search(coord_pattern, wikitext, re.IGNORECASE)
            if match:
                return {"latitude": float(match.group(1)), "longitude": float(match.group(2))}
        except:
            pass

        print(f"      å°è¯•å» OSM æœç´¢: {title} ...")
        osm_coords = self._fetch_coords_from_osm(title)
        if osm_coords:
            return osm_coords

        return {"latitude": None, "longitude": None}
    
    def _extract_coordinates_logic(self, title, wikitext):
        """åˆ†å±‚æ¬¡åæ ‡æå–"""
        print(f"\n      è·å–åæ ‡: {title}")
        
        # 1. æ‰‹åŠ¨åæ ‡ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        if title in MANUAL_COORDS:
            print(f"      âœ“ ä½¿ç”¨æ‰‹åŠ¨åæ ‡")
            coords = MANUAL_COORDS[title].copy()
            coords["source"] = "manual"
            return coords
        
        # 2. Wikiåæ ‡æ¨¡æ¿
        try:
            coord_pattern = r"\{\{[Cc]oord\|([0-9\.]+)\|([0-9\.\-]+)\}\}"
            match = re.search(coord_pattern, wikitext, re.IGNORECASE)
            if match:
                coords = {
                    "latitude": float(match.group(1)),
                    "longitude": float(match.group(2)),
                    "source": "wiki_template"
                }
                print(f"      âœ“ ä»Wikiæ¨¡æ¿è·å–åæ ‡")
                return coords
        except Exception as e:
            print(f"      Wikiæ¨¡æ¿æå–é”™è¯¯: {e}")
            pass
        
        # 4. OSMå¤±è´¥ï¼Œå°è¯•Grid Refï¼ˆç¬¬äºŒå±‚ï¼‰â† é—®é¢˜å¯èƒ½åœ¨è¿™é‡Œï¼
        print(f"      â†’ OSMå¤±è´¥ï¼Œå°è¯•Grid Ref...")
        
        # å…ˆæ£€æŸ¥wikitextæ˜¯å¦æœ‰æ•ˆ
        if not wikitext:
            print(f"      âœ— wikitextä¸ºç©ºï¼Œæ— æ³•æå–Grid Ref")
        elif len(wikitext) < 10:
            print(f"      âœ— wikitextå¤ªçŸ­({len(wikitext)}å­—ç¬¦)ï¼Œå¯èƒ½æœ‰é—®é¢˜")
        else:
            print(f"      wikitexté•¿åº¦: {len(wikitext)}å­—ç¬¦")
        
        grid_coords = self.extract_grid_ref_coords(wikitext)
        if grid_coords:
            print(f"      âœ“ Grid Refæ‰¾åˆ°åæ ‡")
            return grid_coords
        else:
            print(f"      âœ— Grid Refæœªæ‰¾åˆ°åæ ‡")

        # 3. å…ˆå°è¯•OSMæœç´¢ï¼ˆç¬¬ä¸€å±‚ï¼‰
        print(f"      â†’ å°è¯•OSMæœç´¢...")
        osm_coords = self._fetch_coords_from_osm(title)
        if osm_coords:
            osm_coords["source"] = "osm_search"
            print(f"      âœ“ OSMæ‰¾åˆ°åæ ‡")
            return osm_coords
        else:
            print(f"      âœ— OSMæœªæ‰¾åˆ°åæ ‡")
        
        # 5. æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨åæ ‡
        print(f"      â†’ æ‰€æœ‰æ–¹æ³•å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨åæ ‡")
        return {
            "latitude": 54.864, 
            "longitude": -6.268,
            "source": "backup",
            "estimated": True
        }

    def get_full_page_content_via_api(self, page_title):
        try:
            clean_title = self._clean_page_title(page_title)
            
            params = {
                'action': 'query', 
                'prop': 'revisions', 
                'titles': clean_title, 
                'rvprop': 'content', 
                'format': 'json'
            }
            response = self.session.get(self.api_url, params=params, timeout=10)
            data = response.json()
            pages = data['query']['pages']
            
            for page_id, page_data in pages.items():
                if 'revisions' in page_data:
                    content = page_data['revisions'][0]['*']
                    
                    routes_section = self._extract_routes_section(content)
                    
                    # ä¸è¦è½¬æ¢æˆ plain_textï¼Œä¿ç•™åŸå§‹å†…å®¹ï¼
                    # plain_routes_text = self._wikitext_to_plain(routes_section)
                    
                    coordinates = self._extract_coordinates_logic(clean_title, content)
                    crag_type = self._determine_crag_type(content)

                    return {
                        'title': clean_title,
                        'content': content,
                        'routes_section': routes_section,  # æ–°å¢ï¼šåŸå§‹è·¯çº¿éƒ¨åˆ†
                        'coordinates': coordinates,
                        'crag_type': crag_type
                    }

            return {'error': 'é¡µé¢ä¸å­˜åœ¨', 'page_title': clean_title}
            
        except Exception as e:
            print(f"    APIè¯·æ±‚å¤±è´¥: {str(e)}")
            return {'error': str(e), 'page_title': page_title}


    def get_climbing_routes_from_page(self, page_content):
        routes = []
        if not page_content or 'error' in page_content: 
            return routes
        
        # ä½¿ç”¨ routes_sectionï¼Œä¸æ˜¯ plain_textï¼
        routes_section = page_content.get('routes_section', '') or ''
        if not routes_section:
            return routes

        lines = [line.strip() for line in routes_section.split('\n') if line.strip()]
        seen_names = set()

        for line in lines:

            if len(routes) >= 20:
                print(f"    å·²æ”¶é›†20æ¡è·¯çº¿ï¼Œåœæ­¢è§£æ")
                break

            parsed = self._parse_route_line(line)
            if not parsed: 
                continue
            name = parsed['name']
            if name in seen_names: 
                continue
            
            route_data = {
                'name': name,
                'height': parsed['height'],
                'difficulty': parsed['difficulty'],
                'overall_grade': parsed['overall_grade'],
                'technical_grade': parsed['technical_grade'],
                'sub_routes': [],
            }
            # ä½¿ç”¨ routes_sectionï¼Œä¿ç•™HTMLç»“æ„ï¼
            self.enrich_route_data(route_data, routes_section, name)
            routes.append(route_data)
            seen_names.add(name)

        print(f"    æ‰¾åˆ° {len(routes)} æ¡è·¯çº¿")
        return routes
    
    #def get_climbing_routes_from_page(self, page_content):
        routes = []
        if not page_content or 'error' in page_content: 
            return routes
        
        full_content = page_content.get('content', '') or ''
        if not full_content:
            return routes
        
        lines = full_content.split('\n')
        seen_names = set()
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # æ¸…ç†wikitextæ ‡è®°
            line = re.sub(r"''+", '', line)
            line = re.sub(r'\[\[[^|\]]*\|([^\]]+)\]\]', r'\1', line)
            line = re.sub(r'\[\[([^\]]+)\]\]', r'\1', line)
            
            #  1. å…ˆåŒ¹é…å¸¦é«˜åº¦çš„ä¼ ç»Ÿè·¯çº¿
            trad_match = re.search(r'^(.{1,50}?)\s+(\d+)m\s+((?:E[1-9]|HVS|VS|HS|S|VD))', line, re.IGNORECASE)
            if trad_match:
                name = trad_match.group(1).strip()
                height = int(trad_match.group(2))
                grade = trad_match.group(3).strip().upper()
                
                # æ¸…ç†åå­—é‡Œçš„\tå’Œå¤šä½™ç©ºæ ¼
                name = re.sub(r'\s+', ' ', name).strip()
                
                if len(name) >= 2 and name.lower() not in seen_names:
                    route = {
                        'name': name,
                        'height': height,
                        'difficulty': grade,
                        'overall_grade': grade,
                        'technical_grade': None,
                        'sub_routes': [],
                        'first_ascent': 'Unknown',
                        'description': 'No description available'
                    }
                    routes.append(route)
                    seen_names.add(name.lower())
                    print(f"      {name} {height}m {grade}")
                    continue
            
            #  2. å†åŒ¹é…æŠ±çŸ³è·¯çº¿ï¼ˆæ— é«˜åº¦ï¼‰
            boulder_match = re.search(r'^(.{1,50}?)\s+([4-8][abc\+]?)$', line, re.IGNORECASE)
            if boulder_match:
                name = boulder_match.group(1).strip()
                grade = boulder_match.group(2).strip().upper()
                
                name = re.sub(r'\s+', ' ', name).strip()
                
                if len(name) >= 2 and name.lower() not in seen_names:
                    route = {
                        'name': name,
                        'height': None,
                        'difficulty': grade,
                        'overall_grade': grade,
                        'technical_grade': None,
                        'sub_routes': [],
                        'first_ascent': 'Unknown',
                        'description': 'No description available'
                    }
                    routes.append(route)
                    seen_names.add(name.lower())
                    print(f"      {name} {grade}")
        
        print(f"    å…±æ‰¾åˆ° {len(routes)} æ¡è·¯çº¿")
        
        for route in routes[:20]:
            self.enrich_route_data(route, full_content, route['name'])
        
        return routes[:20]
        
    #def _parse_bouldering_routes(self, text):
        """ä¸“é—¨è§£ææŠ±çŸ³è·¯çº¿"""
        routes = []
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        seen_names = set()
        
        for line in lines:
            # æ¸…ç†wikitextæ ‡è®°
            line = re.sub(r"''+", '', line)
            line = re.sub(r'\[\[[^|\]]*\|([^\]]+)\]\]', r'\1', line)
            
            # åŒ¹é…æŠ±çŸ³æ ¼å¼ï¼š"è·¯çº¿å éš¾åº¦"
            match = re.search(r'^(.+?)\s+(4[abc]?|5[abc\+]?|6[abc\+]?|7[abc\+]?|8[abc\+]?)\s*$', line)
            if not match:
                continue
                
            name = match.group(1).strip()
            grade = match.group(2).strip().upper()
            
            if name.lower() in seen_names:
                continue
                
            route = {
                'name': name,
                'height': None,
                'difficulty': grade,
                'overall_grade': grade,
                'technical_grade': None,
                'sub_routes': [],
                'first_ascent': 'Unknown',
                'description': 'No description available'
            }
            routes.append(route)
            seen_names.add(name.lower())
            print(f"      æŠ±çŸ³è·¯çº¿: {name} ({grade})")
        
        print(f"    æ‰¾åˆ° {len(routes)} æ¡æŠ±çŸ³è·¯çº¿")
        return routes[:20]

    #def enrich_route_data(self, route_data, content, route_name):
        """ä¿®å¤ç‰ˆæœ¬ - å‡†ç¡®è·å–è·¯çº¿æè¿°å’Œé¦–æ”€ä¿¡æ¯ï¼Œå¹¶å¤„ç†å¤šæ®µè·¯çº¿"""
        if not content or not route_name:
            return
        
        print(f"      å¤„ç†è·¯çº¿: {route_name}")
        
        # 1. å¯»æ‰¾åŒ…å«è·¯çº¿åçš„æ ‡é¢˜è¡Œï¼ˆå¿…é¡»åŒ…å«é«˜åº¦å’Œéš¾åº¦ï¼‰
        # æ›´ç²¾ç¡®çš„æ¨¡å¼ï¼šè·¯çº¿å é«˜åº¦m éš¾åº¦
        title_pattern = rf'(?:\*\*)?{re.escape(route_name)}\s+(\d+)m\s+([A-Z0-9/]+)(?:\s*\(([^)]+)\))?(?:\*\*)?'
        title_match = re.search(title_pattern, content, re.IGNORECASE)
        
        if not title_match:
            print(f"      æœªæ‰¾åˆ°{route_name}çš„æ ‡é¢˜è¡Œ")
            return
        
        route_title = title_match.group(0)
        print(f"      æ‰¾åˆ°æ ‡é¢˜: {route_title}")
        
        # 2. ä»æ ‡é¢˜ç»“æŸä½ç½®å¼€å§‹
        title_end = title_match.end()
        remaining_content = content[title_end:]
        
        # 3. æ›´æ™ºèƒ½çš„è¾¹ç•Œæ£€æµ‹
        # å…ˆæŸ¥æ‰¾å…¸å‹çš„ä¸‹ä¸€æ¡è·¯çº¿æ¨¡å¼
        next_route_start = len(remaining_content)
        
        # æ¨¡å¼1: æ•°å­—ç¼–å·åè·Ÿè·¯çº¿å (å¦‚ "9. Pat's Route 18m VS")
        pattern1 = rf'\n\s*\d+\.\s+[A-Z][A-Za-z\'\- ]+\s+\d+m\s+[A-Z]'
        # æ¨¡å¼2: è·¯çº¿å é«˜åº¦m éš¾åº¦ (æ— ç¼–å·)
        pattern2 = rf'\n\s*[A-Z][A-Za-z\'\- ]+\s+\d+m\s+[A-Z]'
        # æ¨¡å¼3: **è·¯çº¿å** æ ¼å¼
        pattern3 = rf'\n\s*\*\*[A-Z][A-Za-z\'\- ]+\s+\d+m\s+[A-Z]'
        
        for pattern in [pattern1, pattern2, pattern3]:
            match = re.search(pattern, remaining_content, re.MULTILINE | re.IGNORECASE)
            if match and match.start() > 50:  # ç¡®ä¿ä¸æ˜¯å½“å‰è·¯çº¿çš„ä¸€éƒ¨åˆ†
                # éªŒè¯è¿™ç¡®å®æ˜¯å¦ä¸€æ¡è·¯çº¿
                line_text = remaining_content[match.start():match.end()].strip()
                if route_name.lower() not in line_text.lower():  # ä¸æ˜¯å½“å‰è·¯çº¿
                    next_route_start = min(next_route_start, match.start())
                    print(f"      æ£€æµ‹åˆ°ä¸‹ä¸€æ¡è·¯çº¿: {line_text[:40]}...")
                    break
        
        # 4. å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜æ˜¾çš„ä¸‹ä¸€æ¡è·¯çº¿ï¼Œå°è¯•æ‰¾ä¸‹ä¸€ä¸ªæ•°å­—ç¼–å·
        if next_route_start == len(remaining_content):
            # æŸ¥æ‰¾ä¸‹ä¸€ä¸ªä»¥æ•°å­—åŠ å¥ç‚¹å¼€å¤´çš„è¡Œ
            next_num_match = re.search(r'\n\s*\d+\.\s+', remaining_content, re.MULTILINE)
            if next_num_match and next_num_match.start() > 50:
                next_route_start = min(next_route_start, next_num_match.start())
                print(f"      æ£€æµ‹åˆ°ä¸‹ä¸€ä¸ªæ•°å­—ç¼–å·æ®µè½")
        
        # 5. æå–ä¸“å±å†…å®¹
        route_specific_content = remaining_content[:next_route_start].strip()
        print(f"      ä¸“å±å†…å®¹é•¿åº¦: {len(route_specific_content)} å­—ç¬¦")
        
        if not route_specific_content:
            print(f"      æ²¡æœ‰ä¸“å±å†…å®¹")
            return
        
        # 6. å…ˆå¤„ç†å¤šæ®µè·¯çº¿ï¼ˆä¼˜å…ˆçº§é«˜ï¼‰
        pitch_pattern = r'(\d+)\)\s*(\d+)m\.?\s*(.*?)(?=\s*\d+\)\s*\d+m|$)'
        pitch_matches = list(re.finditer(pitch_pattern, route_specific_content, re.DOTALL | re.IGNORECASE))
        
        if pitch_matches:
            print(f"      å‘ç° {len(pitch_matches)} ä¸ªåˆ†æ®µ")
            route_data['sub_routes'] = []
            
            for i, match in enumerate(pitch_matches):
                pitch_num = int(match.group(1))
                height = int(match.group(2))
                description = match.group(3).strip()
                
                # æ¸…ç†æè¿°æ–‡æœ¬
                description = re.sub(r'\s+', ' ', description)
                
                # å°è¯•æå–è¯¥æ®µçš„éš¾åº¦ç­‰çº§
                pitch_grade = None
                grade_match = re.search(r'\b([A-Z][0-9]?[a-z]?[+-]?|[0-9][abc][+-]?)\b', description)
                if grade_match:
                    pitch_grade = grade_match.group(1)
                
                sub_route = {
                    'pitch_number': pitch_num,
                    'height': height,
                    'description': description[:200],
                    'technical_grade': pitch_grade
                }
                route_data['sub_routes'].append(sub_route)
                print(f"        åˆ†æ®µ {pitch_num}: {height}m - {description[:50]}...")
            
            # ç§»é™¤å·²å¤„ç†çš„åˆ†æ®µå†…å®¹
            last_pitch_end = pitch_matches[-1].end()
            route_specific_content = route_specific_content[last_pitch_end:].strip()
            print(f"      åˆ†æ®µå¤„ç†åå‰©ä½™å†…å®¹é•¿åº¦: {len(route_specific_content)}")
        
        # 7. æå–é¦–æ”€ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        # é¦–æ”€ä¿¡æ¯é€šå¸¸åœ¨ç¬¬ä¸€è¡Œï¼Œæ ¼å¼å¦‚ "I Rea, M Rea. 7/7/1984." æˆ– "T McQueen, A Lyttle. 7/1984."
        fa_pattern = r'^([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*(?:,\s*[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)*\.?\s+(?:\d{1,2}/\d{1,2}/)?\d{4}(?:\.|$))'
        fa_match = re.match(fa_pattern, route_specific_content)
        
        if fa_match:
            first_ascent_text = fa_match.group(1).strip()
            route_data['first_ascent'] = self.clean_text(first_ascent_text)
            print(f"      æå–é¦–æ”€: {route_data['first_ascent'][:60]}...")
            
            # ç§»é™¤é¦–æ”€ä¿¡æ¯
            route_specific_content = route_specific_content[fa_match.end():].strip()
            print(f"      é¦–æ”€å¤„ç†åå‰©ä½™å†…å®¹é•¿åº¦: {len(route_specific_content)}")
        
        # 8. æå–æè¿°ï¼ˆå‰©ä½™å†…å®¹ï¼‰
        if route_specific_content:
            # æŒ‰è¡Œåˆ†å‰²ï¼Œè¿‡æ»¤ç©ºè¡Œ
            lines = [line.strip() for line in route_specific_content.split('\n') if line.strip()]
            
            if lines:
                description_lines = []
                for line in lines:
                    # è·³è¿‡å¤ªçŸ­çš„è¡Œæˆ–å¯èƒ½çš„ä¸‹ä¸€æ¡è·¯çº¿
                    if len(line) < 10 or re.search(r'^\d+\.\s+[A-Z]', line):
                        continue
                    # è·³è¿‡æ˜æ˜¾çš„é¦–æ”€ä¿¡æ¯ï¼ˆå¦‚æœä¹‹å‰æ²¡åŒ¹é…åˆ°ï¼‰
                    if re.search(r'[12]\d{3}\.', line) and re.search(r'[A-Z][a-z]+,', line):
                        continue
                    
                    description_lines.append(line)
                
                if description_lines:
                    description = ' '.join(description_lines)
                    # æ¸…ç†å¤šä½™ç©ºæ ¼
                    description = re.sub(r'\s+', ' ', description).strip()
                    
                    # ç§»é™¤å°¾éƒ¨å¯èƒ½çš„ä¸‹ä¸€æ¡è·¯çº¿å¼•ç”¨
                    description = re.sub(r'\s*\d+\.\s+[A-Z].*$', '', description)
                    description = re.sub(r'\s*[A-Z][a-z]+\s+\d+m\s+[A-Z].*$', '', description)
                    
                    if len(description) > 15:
                        route_data['description'] = description[:400]  # é€‚å½“å¢åŠ é•¿åº¦é™åˆ¶
                        print(f"      æå–æè¿°: {route_data['description'][:80]}...")
        
        # 9. å¦‚æœæ²¡æœ‰æ‰¾åˆ°æè¿°ä½†æœ‰åˆ†æ®µï¼Œåˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªåˆ†æ®µçš„æè¿°ä½œä¸ºæ€»ä½“æè¿°
        if not route_data.get('description') and route_data.get('sub_routes'):
            first_pitch_desc = route_data['sub_routes'][0]['description']
            if len(first_pitch_desc) > 20:
                route_data['description'] = first_pitch_desc[:200]
                print(f"      ä½¿ç”¨ç¬¬ä¸€ä¸ªåˆ†æ®µä½œä¸ºæè¿°: {first_pitch_desc[:60]}...")
        
        # 10. å¦‚æœä»ç„¶æ²¡æœ‰æè¿°ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰è¢«é”™è¯¯æˆªæ–­çš„å†…å®¹
        if not route_data.get('description') and len(route_specific_content) > 0:
            # å¯èƒ½é¦–æ”€æ¨¡å¼ä¸åŒ¹é…ï¼Œå°è¯•æå–ä»»ä½•æœ‰æ„ä¹‰çš„æ–‡æœ¬
            lines = [line.strip() for line in route_specific_content.split('\n') if line.strip() and len(line) > 15]
            if lines:
                description = ' '.join(lines[:2])  # å–å‰ä¸¤è¡Œ
                description = re.sub(r'\s+', ' ', description).strip()
                if len(description) > 20:
                    route_data['description'] = description[:300]
                    print(f"      ä»å‰©ä½™æ–‡æœ¬æå–æè¿°: {description[:60]}...")
        
        print(f"      å®Œæˆå¤„ç† (å­è·¯çº¿: {len(route_data.get('sub_routes', []))}, æè¿°: {'æœ‰' if route_data.get('description') else 'æ— '}, é¦–æ”€: {'æœ‰' if route_data.get('first_ascent') else 'æ— '})")

    #def enrich_route_data(self, route_data, content, route_name):
        """ä¿®å¤ç‰ˆæœ¬ - æ›´ç®€å•é²æ£’åœ°æå–è·¯çº¿ä¿¡æ¯"""
        if not content or not route_name:
            return
        
        print(f"      å¤„ç†è·¯çº¿: {route_name}")
        
        # 1. å¯»æ‰¾åŒ…å«è·¯çº¿åçš„è¡Œï¼ˆæ›´å®½æ¾çš„åŒ¹é…ï¼‰
        # å…ˆæ‰¾åˆ°æ‰€æœ‰åŒ…å«è·¯çº¿åçš„è¡Œ
        lines = content.split('\n')
        route_lines = []
        
        for i, line in enumerate(lines):
            if route_name.lower() in line.lower():
                route_lines.append((i, line.strip()))
        
        if not route_lines:
            print(f"     æœªæ‰¾åˆ°åŒ…å«'{route_name}'çš„è¡Œ")
            return
        
        print(f"      æ‰¾åˆ° {len(route_lines)} è¡ŒåŒ…å«è·¯çº¿å")
        
        # 2. æ‰¾åˆ°ä¸»æ ‡é¢˜è¡Œï¼ˆé€šå¸¸åŒ…å«é«˜åº¦å’Œéš¾åº¦ï¼‰
        main_line = None
        for line_idx, line in route_lines:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«é«˜åº¦å’Œéš¾åº¦
            if re.search(r'\d+m\s+[A-Z]', line):
                main_line = (line_idx, line)
                print(f"      æ‰¾åˆ°ä¸»æ ‡é¢˜è¡Œ: {line[:80]}...")
                break
        
        if not main_line and route_lines:
            main_line = route_lines[0]
        
        # 3. æå–è¯¥è·¯çº¿ä¸“å±çš„å†…å®¹åŒºåŸŸ
        start_line = main_line[0]
        end_line = len(lines)
        
        # æŸ¥æ‰¾ä¸‹ä¸€æ¡è·¯çº¿çš„å¼€å§‹ï¼ˆä¸‹ä¸€ä¸ªåŒ…å«é«˜åº¦å’Œéš¾åº¦çš„è¡Œï¼‰
        for i in range(start_line + 1, len(lines)):
            line = lines[i].strip()
            # è·³è¿‡ç©ºè¡Œ
            if not line:
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸‹ä¸€æ¡è·¯çº¿çš„å¼€å§‹ï¼ˆåŒ…å«é«˜åº¦å’Œéš¾åº¦ä½†ä¸æ˜¯å½“å‰è·¯çº¿ï¼‰
            if re.search(r'\d+m\s+[A-Z]', line) and route_name.lower() not in line.lower():
                # éªŒè¯è¿™ç¡®å®æ˜¯å¦ä¸€æ¡è·¯çº¿
                # ç®€å•çš„éªŒè¯ï¼šè‡³å°‘åŒ…å«ä¸€ä¸ªå•è¯å’Œæ•°å­—
                words = re.findall(r'[A-Za-z\']+', line)
                if words and len(words[0]) > 2:  # è‡³å°‘3ä¸ªå­—æ¯çš„å•è¯
                    end_line = i
                    print(f"      æ£€æµ‹åˆ°ä¸‹ä¸€æ¡è·¯çº¿: {line[:60]}...")
                    break
        
        # 4. æå–ä¸“å±å†…å®¹
        route_content_lines = lines[start_line:end_line]
        route_content = '\n'.join(route_content_lines).strip()
        
        print(f"      ä¸“å±å†…å®¹é•¿åº¦: {len(route_content)} å­—ç¬¦")
        
        if not route_content:
            print(f"      æ²¡æœ‰ä¸“å±å†…å®¹")
            return
        
        # 5. æå–é¦–æ”€ä¿¡æ¯ï¼ˆå¤šç§æ ¼å¼ï¼‰
        first_ascent = self._extract_first_ascent(route_content)
        if first_ascent:
            route_data['first_ascent'] = first_ascent
            print(f"      æå–é¦–æ”€: {first_ascent[:60]}...")
        
        # 6. æå–åˆ†æ®µä¿¡æ¯
        sub_routes = self._extract_sub_routes(route_content)
        if sub_routes:
            route_data['sub_routes'] = sub_routes
            print(f"      æå–åˆ†æ®µ: {len(sub_routes)} ä¸ª")
        
        # 7. æå–æè¿°ï¼ˆå‰©ä½™å†…å®¹ï¼‰
        description = self._extract_description(route_content, first_ascent, sub_routes)
        if description:
            route_data['description'] = description[:400]
            print(f"      æå–æè¿°: {description[:80]}...")
        
        print(f"      å®Œæˆå¤„ç† (å­è·¯çº¿: {len(route_data.get('sub_routes', []))}, æè¿°: {'æœ‰' if route_data.get('description') else 'æ— '}, é¦–æ”€: {'æœ‰' if route_data.get('first_ascent') else 'æ— '})")

    def enrich_route_data(self, route_data, content, route_name):
        """é‡å†™ - é’ˆå¯¹wikitextæ ¼å¼"""
        if not content or not route_name:
            return
        
        print(f"      å¤„ç†è·¯çº¿: {route_name}")
        
        # 1. åœ¨wikitextä¸­å®šä½è·¯çº¿
        lines = content.split('\n')
        
        # æ‰¾åˆ°è·¯çº¿æ ‡é¢˜è¡Œ
        route_line_index = -1
        route_line_text = ""
        
        for i, line in enumerate(lines):
            # ç§»é™¤wikitextæ ‡è®°åæ£€æŸ¥
            line_clean = re.sub(r"''+", '', line)
            line_clean = re.sub(r'\[\[[^\]]+\]\]', '', line_clean)
            
            if route_name.lower() in line_clean.lower():
                # ç¡®è®¤è¿™æ˜¯è·¯çº¿æ ‡é¢˜è¡Œï¼ˆåŒ…å«é«˜åº¦å’Œéš¾åº¦ï¼‰
                if re.search(r'\d+\s*m', line_clean, re.IGNORECASE):
                    if re.search(r'\b(E[1-9]|HVS|VS|HS|S|VD|[1-9][abc])\b', line_clean, re.IGNORECASE):
                        route_line_index = i
                        route_line_text = line
                        print(f"      æ‰¾åˆ°è·¯çº¿è¡Œ[{i}]: {line_clean[:100]}")
                        break
        
        if route_line_index == -1:
            print(f"     æœªæ‰¾åˆ°è·¯çº¿: {route_name}")
            route_data['first_ascent'] = "Unknown"
            route_data['description'] = "No description available"
            return
        
        # 2. æå–é¦–æ”€ä¿¡æ¯
        first_ascent = "Unknown"
        
        # åœ¨è·¯çº¿è¡Œä¹‹åæŸ¥æ‰¾é¦–æ”€ä¿¡æ¯
        for i in range(route_line_index + 1, min(route_line_index + 5, len(lines))):
            line = lines[i].strip()
            
            # è·³è¿‡ç©ºè¡Œå’Œwikiæ ‡è®°è¡Œ
            if not line or line.startswith(('*', '#', ':', ';', '{', '|')):
                continue
            
            line_clean = re.sub(r"''+", '', line)
            line_clean = re.sub(r'\[\[[^\]]+\]\]', '', line_clean)
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«é¦–æ”€ç‰¹å¾
            if re.search(r'[A-Z][a-z]+', line_clean):  # åŒ…å«äººå
                if re.search(r'\d{4}', line_clean):  # åŒ…å«å¹´ä»½
                    first_ascent = re.split(r'<br\s*/?>', line_clean.strip())[0]
                    print(f"      æ‰¾åˆ°é¦–æ”€[{i}]: {first_ascent[:80]}")
                    break
            
            # æ£€æŸ¥FA:æ ‡è®°
            if re.search(r'FA[:\.]|First\s+Ascent', line, re.IGNORECASE):
                fa_match = re.search(r'FA[:\.]\s*(.+?)(?=\n|$|<br>)', line, re.IGNORECASE)
                if fa_match:
                    first_ascent = fa_match.group(1).strip()
                    print(f"      æ‰¾åˆ°FAæ ‡è®°: {first_ascent[:80]}")
                    break
        
        route_data['first_ascent'] = first_ascent if first_ascent != "Unknown" else "Unknown"
        
        # 3. æå–æè¿°ä¿¡æ¯
        description_lines = []
        
        # ä»è·¯çº¿è¡Œä¸‹ä¸€è¡Œå¼€å§‹ï¼Œç›´åˆ°é‡åˆ°ä¸‹ä¸€ä¸ªè·¯çº¿æ ‡é¢˜
        for i in range(route_line_index + 1, len(lines)):
            line = lines[i].strip()
            
            if not line:
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸‹ä¸€æ¡è·¯çº¿
            if re.search(r'[A-Z][a-z]+', line_clean):  # åŒ…å«äººå
                if re.search(r'\d{4}', line_clean):  # åŒ…å«å¹´ä»½
                    first_ascent = line_clean.strip().split('<br>')[0]
                    print(f"      æ‰¾åˆ°é¦–æ”€[{i}]: {first_ascent[:80]}")
                    break
            
            # è·³è¿‡wikiæ ‡è®°è¡Œ
            if line.startswith(('==', '*', '#', ':', ';', '{', '|', '[[Category', '[[File')):
                continue
            
            # è·³è¿‡å·²ç»æå–ä¸ºé¦–æ”€çš„è¡Œ
            if first_ascent != "Unknown" and first_ascent in line:
                continue
            
            # æ¸…ç†wikitextæ ‡è®°
            line_clean = re.sub(r"''+", '', line)
            line_clean = re.sub(r'\[\[[^|\]]*\|([^\]]+)\]\]', r'\1', line_clean)
            line_clean = re.sub(r'\[\[([^\]]+)\]\]', r'\1', line_clean)
            line_clean = re.sub(r'\{\{[^\}]+\}\}', '', line_clean)
            line_clean = re.sub(r'<[^>]+>', '', line_clean)
            line_clean = re.sub(r'\s+', ' ', line_clean).strip()
            
            # åªä¿ç•™æœ‰æ„ä¹‰çš„æè¿°æ–‡æœ¬
            if len(line_clean) > 15:
                # è·³è¿‡çœ‹èµ·æ¥åƒè·¯çº¿æ ‡é¢˜çš„è¡Œ
                if not re.search(r'^\d+[\.\)]?\s*\w+\s+\d+m', line_clean):
                    description_lines.append(line_clean)
            
            # é™åˆ¶æè¿°é•¿åº¦
            if len(' '.join(description_lines)) > 500:
                break
        
        if description_lines:
            description = ' '.join(description_lines)
            description = re.sub(r'\s+', ' ', description).strip()
            route_data['description'] = description[:500]
            print(f"      æ‰¾åˆ°æè¿°: {description[:100]}...")
        else:
            route_data['description'] = None
        
        print(f"      å®Œæˆ: {route_name}")
        
    def _extract_first_ascent(self, text):
        """å¤šç§æ ¼å¼æå–é¦–æ”€ä¿¡æ¯"""
        patterns = [
            r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*(?:,\s*[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)*\.?\s*(?:\d{1,2}/\d{1,2}/)?\d{4}\.?)\b',
            r'([A-Z]\s+[A-Z][a-z]+(?:,\s*[A-Z]\s+[A-Z][a-z]+)*\.\s*(?:\d{1,2}/)?\d{4}\.?)\b',
            r'(?:FA[:\s]+)([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*(?:,\s*[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)*\s+(?:\d{1,2}/\d{1,2}/)?\d{4})',
            r'(?:First\s+ascent[:\s]+)([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*(?:,\s*[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)*[^.]+\d{4}\.?)\b',
            r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*[^.]+\d{4})\.',
        ]
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
            if match:
                fa_text = match.group(1).strip()
                fa_text = re.sub(r'\s+', ' ', fa_text)
                return fa_text
        
        return None

    def _extract_sub_routes(self, text):
        """æå–åˆ†æ®µä¿¡æ¯"""
        sub_routes = []
        
        patterns = [
            r'(\d+)\)\s*(\d+)m\.?\s*(.*?)(?=\s*\d+\)\s*\d+m|$)',
            r'(\d+)/\s*(\d+)m\.?\s*(.*?)(?=\s*\d+/\s*\d+m|$)',
            r'Pitch\s*(\d+)[:\-]\s*(\d+)m\.?\s*(.*?)(?=\s*Pitch\s*\d+[:\-]|$)',
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE | re.DOTALL)
            for match in matches:
                pitch_num = int(match.group(1))
                height = int(match.group(2))
                description = match.group(3).strip()
                
                description = re.sub(r'\s+', ' ', description)
                
                technical_grade = None
                grade_match = re.search(r'\b([A-Z][0-9]?[a-z]?[+-]?|[0-9][abc][+-]?)\b', description)
                if grade_match:
                    technical_grade = grade_match.group(1)
                
                sub_route = {
                    'pitch_number': pitch_num,
                    'height': height,
                    'description': description[:200],
                    'technical_grade': technical_grade
                }
                sub_routes.append(sub_route)
        
        return sub_routes

    def _extract_description(self, text, first_ascent, sub_routes):
        cleaned_text = text
        
        if first_ascent:
            cleaned_text = re.sub(re.escape(first_ascent), '', cleaned_text, flags=re.IGNORECASE)
        
        if sub_routes:
            for sub in sub_routes:
                pattern = rf"{sub['pitch_number']}\)\s*{sub['height']}m\.?\s*{re.escape(sub['description'][:50])}"
                cleaned_text = re.sub(pattern, '', cleaned_text, flags=re.DOTALL)
        
        lines = cleaned_text.split('\n')
        
        meaningful_lines = []
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            if re.match(r'^\d+[\)/]', line):
                continue
            if re.search(r'^\d+m\s+[A-Z]', line):
                continue
            if len(line) < 10:
                continue
            if re.search(r'[12]\d{3}\b', line) and re.search(r'[A-Z][a-z]+,', line):
                continue
            
            meaningful_lines.append(line)
        
        if meaningful_lines:
            description = ' '.join(meaningful_lines)
            description = re.sub(r'\s+', ' ', description).strip()
            
            description = re.sub(r'\s*\*\*.*$', '', description)
            description = re.sub(r'\s*\[\[.*$', '', description)
            
            return description if len(description) > 15 else None
        
        return None

    def parse_pitch_line(self, route_data, line):
        """è¾…åŠ©æ–¹æ³•ï¼šè§£æå•ä¸ªåˆ†æ®µè¡Œ"""
        pitch_match = re.match(r'(\d+)[)/]\.?\s*(\d+)m\.?\s*(.*)', line)
        if pitch_match:
            sub_route = {
                'pitch_number': int(pitch_match.group(1)), 
                'height': int(pitch_match.group(2)), 
                'description': self.clean_text(pitch_match.group(3))
            }
            grade_match = re.search(r'\b([A-Z][0-9]?[a-z]?[+-]?|[0-9][abc][+-]?)\b', sub_route['description'])
            if grade_match:
                sub_route['technical_grade'] = grade_match.group(1)
            
            route_data['sub_routes'].append(sub_route)
            return True
        return False

    def clean_text(self, text):
        if not text: return ""
        cleaned = re.sub(r'\s+', ' ', text)
        return cleaned.strip()

    def _clean_page_title(self, page_title):
        cleaned = urllib.parse.unquote(page_title)
        cleaned = cleaned.replace('%27', "'").replace('%28', '(').replace('%29', ')')
        return cleaned

    def _is_valid_climbing_site(self, text, href):
        if not text or len(text) < 3: return False
        exclude_texts = ['edit', 'search', 'category', 'file', 'template', 'user', 'special', 'talk', 'main page', 'discussion', 'create account', 'log in', 'navigation', 'page', 'read', 'view source', 'history']
        text_lower = text.lower()
        if any(exclude in text_lower for exclude in exclude_texts): return False
        if not href.startswith('/index.php?title='): return False
        if text.startswith('Co. '): return False
        return True

    def _extract_page_title(self, href):
        if 'title=' in href: return href.split('title=')[1].split('&')[0]
        return href.replace('/', '')

    def collect_all_data(self):
        print("å¼€å§‹å®Œæ•´çš„çˆ±å°”å…°æ”€å²©æ•°æ®æ”¶é›†...")
        all_structure = self.get_all_counties_and_sites_via_scraping()
        if not all_structure: return {}

        all_complete_data = {}
        for county, county_data in all_structure.items():
            print(f"\nå¤„ç†éƒ¡: {county}")
            all_complete_data[county] = {'county_info': county_data['county_info'], 'climbing_sites': []}
            
            for site in county_data['climbing_sites']:
                print(f"  æ­£åœ¨å¤„ç†: {site['name']}")
                page_content = self.get_full_page_content_via_api(site['page_title'])
                routes = self.get_climbing_routes_from_page(page_content)

                site_data = {
                    'name': site['name'], 'page_title': site['page_title'], 'url': site['url'],
                    'routes': routes, 'routes_count': len(routes),
                    'coordinates': page_content.get('coordinates', {'latitude': None, 'longitude': None}),
                    'climbing_type': page_content.get('crag_type', 'Unknown')
                }
                all_complete_data[county]['climbing_sites'].append(site_data)
                print(f"  {site_data['climbing_type']} | {site_data['coordinates']}")
                time.sleep(1)
        return all_complete_data

    def save_complete_data(self, data, filename='complete_irish_climbing_data.json'):
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        print(f"\nå®Œæ•´æ•°æ®å·²ä¿å­˜åˆ° {filename}")

    def generate_summary(self, data):
        total_counties = len(data)
        total_sites = 0; total_routes = 0
        for county, county_data in data.items():
            total_sites += len(county_data['climbing_sites'])
            for site in county_data['climbing_sites']:
                total_routes += len(site['routes'])
        print(f"\n æœ€ç»ˆæ•°æ®æ‘˜è¦: éƒ¡ {total_counties} | ç«™ç‚¹ {total_sites} | è·¯çº¿ {total_routes}")

    def collect_county_data(self, county_keyword: str, max_sites: int = None):
        print(f"åªæ”¶é›†åŒ…å«å…³é”®å­— '{county_keyword}' çš„éƒ¡çš„æ•°æ®...")
        all_structure = self.get_all_counties_and_sites_via_scraping()
        
        selected = {}
        for county, data in all_structure.items():
            if county_keyword.lower() in county.lower():
                selected[county] = data
        
        if not selected:
            return {}

        all_complete_data = {}
        for county, county_data in selected.items():
            print(f"\nå¤„ç†éƒ¡: {county}")
            all_complete_data[county] = {'county_info': county_data['county_info'], 'climbing_sites': []}
            
            sites = county_data['climbing_sites']
            if max_sites is not None:
                sites = sites[:max_sites]

            for site in sites:
                print(f"  æ­£åœ¨å¤„ç†: {site['name']}")
                page_content = self.get_full_page_content_via_api(site['page_title'])
                routes = self.get_climbing_routes_from_page(page_content)

                site_data = {
                    'name': site['name'], 
                    'page_title': site['page_title'], 
                    'climbing_type': page_content.get('crag_type', 'Unknown'),
                    'url': site['url'],
                    'routes': routes, 
                    'routes_count': len(routes),
                    'coordinates': page_content.get('coordinates', {'latitude': None, 'longitude': None}),
                }
                all_complete_data[county]['climbing_sites'].append(site_data)
                print(f"   {site_data['climbing_type']} | {site_data['coordinates']}")
                time.sleep(1)
                
        return all_complete_data

if __name__ == "__main__":
    collector = IrishClimbingRobust()

    counties_list = ["Antrim", "Armagh", "Carlow", "Cavan", "Clare", "Cork", "Derry", 
                    "Donegal", "Down", "Dublin", "Fermanagh", "Galway", "Kerry", 
                    "Kildare", "Kilkenny", "Laois", "Leitrim", "Limerick", "Longford", 
                    "Louth", "Mayo", "Meath", "Monaghan", "Offaly", "Roscommon", 
                    "Sligo", "Tipperary", "Tyrone", "Waterford", "Westmeath", 
                    "Wexford", "Wicklow"]

    for county in counties_list:
        print(f"\nå¤„ç†éƒ¡: {county}")
        county_data = collector.collect_county_data(county, max_sites=None)
        
        if county_data:
            collector.save_complete_data(county_data, f'{county.lower()}_all_data.json')
            collector.generate_summary(county_data)
            print(f"{county} æ•°æ®ä¿å­˜æˆåŠŸ")
        else:
            print(f"æœªæ‰¾åˆ° {county} éƒ¡çš„æ•°æ®")
        
        time.sleep(1.5)